#################################################################
#                            DEFAULTS                           #
#################################################################

x-defaults: &defaults
  x-dummy: ""
  # Putting the anchor in this file ensures it's a valid YAML file for Renovate Bot
  #VAR:lookup('ansible.builtin.file', 'files/docker-compose-defaults.yml',) | indent(width=2):VAR#

x-extra-hosts-docker-host: &extra-hosts-docker-host
  extra_hosts:
    - "host.docker.internal:host-gateway"

x-extra-hosts-wireguard: &extra-hosts-wireguard
  extra_hosts:
    #BLOCK:- for wg_peer in (app_wireguard[0].peers | selectattr("host", "defined")) :BLOCK#
    - "#VAR:wg_peer.host:VAR#.wireguard:#VAR:wg_peer.allowed_ips[0] | regex_replace('/\\d+$', ''):VAR#"
    #BLOCK: endfor :BLOCK#

x-glitchtip-env: &glitchtip-env
  TZ: "#VAR:general_timezone:VAR#"
  PORT: "8080"
  DATABASE_URL: "postgres://glitchtip:#VAR:app_glitchtip_db_pass:VAR#@glitchtip-db:5432/glitchtip"
  REDIS_HOST: "glitchtip-redis"
  SECRET_KEY: "#VAR:app_glitchtip_secret_key:VAR#"
  EMAIL_URL: "smtp://#VAR:mailjet_username:VAR#:#VAR:mailjet_password:VAR#@#VAR:mailjet_host:VAR#:#VAR:mailjet_port_starttls:VAR#"
  DEFAULT_FROM_EMAIL: "glitchtip@#VAR:general_domain_default:VAR#"
  GLITCHTIP_DOMAIN: "https://glitchtip.#VAR:general_domain_default:VAR#"

x-healthcheck-elasticsearch: &healthcheck-elasticsearch
  healthcheck:
    test:
      [
        "CMD-SHELL",
        "curl --silent --fail localhost:9200/_cluster/health || exit 1",
      ]
    interval: 30s
    timeout: 10s
    retries: 5

x-healthcheck-mongo: &healthcheck-mongo
  healthcheck:
    test:
      [
        "CMD-SHELL",
        "mongosh --quiet localhost --eval 'quit(db.runCommand({ ping: 1 }).ok ? 0 : 1)'",
      ]
    interval: 30s
    timeout: 10s
    retries: 5

x-healthcheck-postgres: &healthcheck-postgres
  healthcheck:
    test:
      [
        "CMD-SHELL",
        "sh -c 'pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}'",
      ]
    interval: 30s
    timeout: 10s
    retries: 5

x-fix-prettier-formatting:

#################################################################
#                            SERVICES                           #
#################################################################
services:
  # =========================
  # =         PROXY         =
  # =========================
  # Having 2 reverse proxies (1 on host network and 1 on bridge network)
  # allows to have the real client IP's available inside the bridged
  # Treafik instance (using PROXY protocol). In case you would only have
  # a single reverse proxy on the host network, you would loose all the convenience
  # of the Traefik Docker provider support (not having to expose each service on a host
  # port and auto-configuration of the services/loadbalancers).
  nginx:
    <<: *defaults
    image: docker.io/library/nginx:alpine
    container_name: nginx
    network_mode: host
    volumes:
      - ./nginx/default.conf:/etc/nginx/nginx.conf:ro

  traefik:
    <<: *defaults
    image: docker.io/library/traefik:latest
    container_name: traefik
    command:
      # - "--log.level=DEBUG"
      - "--accesslog.filepath=/access-logs/access.log" # Used by CrowdSec
      - "--providers.file.directory=/conf"
      - "--providers.file.watch=true"
      - "--entrypoints.websecure.address=:443"
      - "--entrypoints.websecure.proxyProtocol.trustedIPs=#VAR:network_any_private:VAR#"
      - "--entrypoints.websecure.http.middlewares=secure-https@file"
      - "--entrypoints.web.address=:80"
      - "--entrypoints.web.proxyProtocol.trustedIPs=#VAR:network_any_private:VAR#"
      - "--entrypoints.web.http.redirections.entryPoint.to=websecure"
      - "--entrypoints.web.http.redirections.entryPoint.scheme=https"
      - "--entrypoints.mqtt.address=:8883"
      - "--entrypoints.mqtt.proxyProtocol.trustedIPs=#VAR:network_any_private:VAR#"
      - "--certificatesresolvers.le-dns.acme.dnschallenge=true"
      - "--certificatesresolvers.le-dns.acme.dnschallenge.provider=desec"
      - "--certificatesresolvers.le-dns.acme.email=#VAR:general_mail_admin:VAR#@#VAR:general_domain_default:VAR#"
      - "--certificatesresolvers.le-dns.acme.storage=/letsencrypt/acme-dns.json"
      - "--certificatesresolvers.le-tls.acme.tlsChallenge=true"
      - "--certificatesresolvers.le-tls.acme.email=#VAR:general_mail_admin:VAR#@#VAR:general_domain_default:VAR#"
      - "--certificatesresolvers.le-tls.acme.storage=/letsencrypt/acme-tls.json"
      - "--certificatesresolvers.le-tls-staging.acme.tlsChallenge=true"
      - "--certificatesresolvers.le-tls-staging.acme.email=#VAR:general_mail_admin:VAR#@#VAR:general_domain_default:VAR#"
      - "--certificatesresolvers.le-tls-staging.acme.storage=/letsencrypt/acme-tls-staging.json"
      - "--certificatesResolvers.le-tls-staging.acme.caServer=https://acme-staging-v02.api.letsencrypt.org/directory"
      - "--api=true"
      - "--ping.manualrouting=true"
    ports:
      - 127.0.0.1:2080:80
      - 127.0.0.1:2443:443
      - 127.0.0.1:28883:8883
    networks:
      - traefik
    volumes:
      - ./traefik:/conf:ro
      - traefik-cert:/letsencrypt
      - traefik-access-logs:/access-logs
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      DESEC_TOKEN: "#VAR:app_traefik_desec_token:VAR#"

  oauth2-proxy:
    <<: *defaults
    image: quay.io/oauth2-proxy/oauth2-proxy:latest
    container_name: oauth2-proxy
    command: >
      --http-address=0.0.0.0:4180
      --reverse-proxy
      --provider=keycloak-oidc
      --oidc-issuer-url=#VAR:app_oauth2_proxy_issuer_url:VAR#
      --client-id=#VAR:app_oauth2_proxy_client_id:VAR#
      --client-secret=#VAR:app_oauth2_proxy_client_secret:VAR#
      --code-challenge-method=S256
      --email-domain=*
      --cookie-domain=#VAR:general_domain_default:VAR#
      --cookie-secret=#VAR:app_oauth2_proxy_cookie_secret:VAR#
    networks:
      - traefik
    environment:
      TZ: "#VAR:general_timezone:VAR#"

  # Since the login page handling of OAuth2 Proxy is based on HTTP
  # status code 401, it might conflict with an upstream reply.
  # Therefore, this helper translates an error response of OAuth2 Proxy
  # to status code 601. This prevents any conflicts.
  oauth2-proxy-helper:
    <<: *defaults
    image: docker.io/library/caddy:2
    container_name: oauth2-proxy-helper
    networks:
      - traefik
    volumes:
      - ./oauth2-proxy/helper/Caddyfile:/etc/caddy/Caddyfile
      - oauth2-proxy-helper-config:/config
      - oauth2-proxy-helper-data:/data
    environment:
      TZ: "#VAR:general_timezone:VAR#"

  # =========================
  # =    HOME AUTOMATION    =
  # =========================
  ha:
    <<: *defaults
    image: ghcr.io/home-assistant/home-assistant:stable
    container_name: ha
    networks:
      - traefik
      - home-automation
    volumes:
      - "./home-automation/home-assistant/automations.yaml:/config/automations.yaml"
      - "./home-automation/home-assistant/configuration.yaml:/config/configuration.yaml"
      - "#VAR:general_path_appdata:VAR#/home-automation/home-assistant/config:/config"
    environment:
      TZ: "#VAR:general_timezone:VAR#"

  ha-zigbee2mqtt:
    <<: *defaults
    image: ghcr.io/koenkk/zigbee2mqtt:latest
    container_name: ha-zigbee2mqtt
    networks:
      - home-automation
    depends_on:
      ha-mosquitto:
        condition: service_started
    volumes:
      - "#VAR:general_path_appdata:VAR#/home-automation/zigbee2mqtt:/app/data"
      - "./home-automation/zigbee2mqtt/configuration.yaml:/app/data/configuration.yaml"
    devices:
      - "/dev/serial/by-id/usb-Silicon_Labs_slae.sh_cc2652rb_stick_-_slaesh_s_iot_stuff_00_12_4B_00_22_98_86_5C-if00-port0:/dev/zigbee"
    environment:
      TZ: "#VAR:general_timezone:VAR#"

  ha-mosquitto:
    <<: *defaults
    image: docker.io/library/eclipse-mosquitto:latest
    container_name: ha-mosquitto
    networks:
      - traefik
      - home-automation
    volumes:
      - "./home-automation/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf:ro"
      - "./home-automation/mosquitto/password_file:/mosquitto/config/password_file"
      - "ha-mosquitto-data:/mosquitto/data/"
    environment:
      TZ: "#VAR:general_timezone:VAR#"

  # =========================
  # =        MINIFLUX       =
  # =========================
  miniflux:
    <<: *defaults
    image: docker.io/miniflux/miniflux:latest
    container_name: miniflux
    networks:
      - traefik
      - miniflux
    depends_on:
      miniflux-db:
        condition: service_healthy
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      DATABASE_URL: "postgres://miniflux:#VAR:app_miniflux_db_pass:VAR#@miniflux-db/miniflux?sslmode=disable"
      RUN_MIGRATIONS: "1"
      CREATE_ADMIN: "1"
      ADMIN_USERNAME: "#VAR:app_miniflux_admin_user:VAR#"
      ADMIN_PASSWORD: "#VAR:app_miniflux_admin_pass:VAR#"
      OAUTH2_PROVIDER: "oidc"
      OAUTH2_OIDC_DISCOVERY_ENDPOINT: "#VAR:app_miniflux_oidc_url:VAR#"
      OAUTH2_CLIENT_ID: "#VAR:app_miniflux_oidc_client_id:VAR#"
      OAUTH2_CLIENT_SECRET: "#VAR:app_miniflux_oidc_client_secret:VAR#"
      OAUTH2_REDIRECT_URL: "https://feed.#VAR:general_domain_default:VAR#/oauth2/oidc/callback"

  miniflux-db:
    <<: [*defaults, *healthcheck-postgres]
    # See https://github.com/miniflux/v2/blob/main/contrib/docker-compose/basic.yml
    image: docker.io/library/postgres:13-alpine
    container_name: miniflux-db
    networks:
      - miniflux
    volumes:
      - "#VAR:general_path_appdata:VAR#/miniflux/db:/var/lib/postgresql/data"
      - "miniflux-db-dump:/backup"
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      POSTGRES_DB: "miniflux"
      POSTGRES_USER: "miniflux"
      POSTGRES_PASSWORD: "#VAR:app_miniflux_db_pass:VAR#"

  # =========================
  # =       MULTIMEDIA      =
  # =========================
  plex:
    <<: *defaults
    image: docker.io/plexinc/pms-docker:latest
    container_name: plex
    network_mode: host
    volumes:
      - "/data/bulk/media:/data/media"
      - "/data/important/Photos:/data/Photos"
      - "#VAR:general_path_appdata:VAR#/plex/config:/config"
      - "#VAR:general_path_appdata:VAR#/plex/transcode:/transcode"
    # devices: # for QuickSync support
    #   - "/dev/dri/card0:/dev/dri/card0"
    #   - "/dev/dri/renderD128:/dev/dri/renderD128"
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      PLEX_UID: "#VAR:ansible_real_user_id:VAR#"
      PLEX_GID: "#VAR:ansible_real_group_id:VAR#"
    deploy:
      resources:
        limits:
          cpus: "7"
          memory: 4G

  jellyfin:
    <<: *defaults
    image: docker.io/jellyfin/jellyfin:latest
    container_name: jellyfin
    user: "#VAR:ansible_real_user_id:VAR#:#VAR:ansible_real_group_id:VAR#"
    group_add: # By id as these may not exist within the container. Needed to provide permissions to the VAAPI Devices
      - "107" # Group "render"
      - "44" # Group "video"
    devices:
      # VAAPI Devices
      - /dev/dri/renderD128:/dev/dri/renderD128
      - /dev/dri/card0:/dev/dri/card0
    networks:
      - traefik
    ports:
      - "8096:8096"
    volumes:
      - "/data/bulk/media:/media"
      - "/data/important/Photos:/Photos"
      - "#VAR:general_path_appdata:VAR#/jellyfin/config:/config"
      - "#VAR:general_path_appdata:VAR#/jellyfin/cache:/cache"
    deploy:
      resources:
        limits:
          cpus: "7"
          memory: 4G

  # =========================
  # =   TINY MEDIA MANAGER  =
  # =========================
  tmm:
    <<: *defaults
    image: docker.io/romancin/tinymediamanager:latest
    container_name: tmm
    networks:
      - traefik
    volumes:
      - "/data/bulk/media:/media"
      - "#VAR:general_path_appdata:VAR#/tmm/config:/config"
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      USER_ID: "#VAR:ansible_real_user_id:VAR#"
      GROUP_ID: "#VAR:ansible_real_group_id:VAR#"
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: 2G

  # =========================
  # =       SYNCTHING       =
  # =========================
  syncthing:
    <<: *defaults
    image: docker.io/syncthing/syncthing:1
    container_name: syncthing
    hostname: "#VAR:ansible_hostname:VAR#"
    networks:
      - traefik
    ports:
      - "22000:22000"
      - "22000:22000/udp"
    volumes:
      - "#VAR:general_path_appdata:VAR#/syncthing/config:/var/syncthing"
      - "/data/bulk/media/Music:/data/music"
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      PUID: "#VAR:ansible_real_user_id:VAR#"
      PGID: "#VAR:ansible_real_user_id:VAR#"

  # =========================
  # =         SAMBA         =
  # =========================
  samba:
    <<: *defaults
    image: docker.io/dperson/samba:latest
    container_name: samba
    ports:
      - "445:445"
    volumes:
      - "/data/bulk/ps2-roms:/data/ps2"
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      USERID: "#VAR:ansible_real_user_id:VAR#"
      GROUPID: "#VAR:ansible_real_group_id:VAR#"
    command: >
      samba.sh -S
      -s "ps2;/data/ps2;yes;no;yes"

  # =========================
  # =          NFS          =
  # =========================
  nfs:
    <<: *defaults
    image: docker.io/erichough/nfs-server:latest
    container_name: nfs
    privileged: true # Required for mount several filesystems inside the container to support its operation
    ports:
      - "111:111"
      - "111:111/udp"
      - "2049:2049"
      - "2049:2049/udp"
      - "32765:32765"
      - "32765:32765/udp"
      - "32767:32767"
      - "32767:32767/udp"
    volumes:
      - "/data/bulk/media:/data/media"
      - "/data/important/Photos:/data/photos"
      - "./nfs/exports:/etc/exports:ro"
    environment:
      TZ: "#VAR:general_timezone:VAR#"

  # =========================
  # =       NEXTCLOUD       =
  # =========================
  nextcloud:
    <<: *defaults
    build: ./nextcloud
    container_name: nextcloud
    hostname: nextcloud
    networks:
      - traefik
      - nextcloud
    depends_on:
      nc-db:
        condition: service_healthy
    volumes:
      - "#VAR:general_path_appdata:VAR#/nextcloud/nextcloud/config:/var/www/html"
      - "#VAR:general_path_appdata:VAR#/nextcloud/nextcloud/data:/var/www/html/data"
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      MYSQL_DATABASE: "nextcloud"
      MYSQL_USER: "nextcloud"
      MYSQL_PASSWORD: "#VAR:app_nextcloud_db_pass_nextcloud:VAR#"
      MYSQL_HOST: "nc-db"
    deploy:
      resources:
        limits:
          cpus: "7"
          memory: 4G

  nc-db:
    <<: *defaults
    # Nextcloud currently only supports up to 10.6.
    # See https://docs.nextcloud.com/server/29/admin_manual/installation/system_requirements.html
    image: docker.io/library/mariadb:10.6
    command: --transaction-isolation=READ-COMMITTED --log-bin=ROW --innodb-read-only-compressed=OFF
    container_name: nc-db
    networks:
      - nextcloud
    volumes:
      - "#VAR:general_path_appdata:VAR#/nextcloud/mariadb/data:/var/lib/mysql"
      - nc-db-dump:/backup
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      MARIADB_ROOT_PASSWORD: "#VAR:app_nextcloud_db_pass_root:VAR#"
      MARIADB_PASSWORD: "#VAR:app_nextcloud_db_pass_nextcloud:VAR#"
      MARIADB_DATABASE: "nextcloud"
      MARIADB_USER: "nextcloud"
      MARIADB_AUTO_UPGRADE: "1"
    healthcheck: # Based on https://mariadb.com/kb/en/using-healthcheck-sh/
      test: ["CMD", "healthcheck.sh", "--connect", "--innodb_initialized"]
      start_period: 10s
      interval: 10s
      timeout: 5s
      retries: 3

  # =========================
  # =        TORRENT        =
  # =========================
  transmission:
    <<: *defaults
    image: docker.io/linuxserver/transmission:latest
    container_name: transmission
    network_mode: service:transmission-vpn
    volumes:
      - "#VAR:general_path_appdata:VAR#/transmission/config:/config"
      - "#VAR:general_path_appdata:VAR#/transmission/downloads:/downloads/incomplete"
      - "/data/bulk/media/Nazien:/downloads/complete"
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      PUID: "#VAR:ansible_real_user_id:VAR#"
      PGID: "#VAR:ansible_real_group_id:VAR#"

  transmission-vpn:
    <<: *defaults
    image: ghcr.io/qdm12/gluetun:latest
    container_name: transmission-vpn
    cap_add:
      - NET_ADMIN
    networks:
      - traefik
    devices:
      - /dev/net/tun:/dev/net/tun
    volumes:
      - "#VAR:general_path_appdata:VAR#/gluetun/config:/gluetun"
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      VPN_SERVICE_PROVIDER: mullvad
      VPN_TYPE: wireguard
      WIREGUARD_PRIVATE_KEY: "#VAR:app_transmission_wireguard_private_key:VAR#"
      WIREGUARD_ADDRESSES: "#VAR:app_transmission_wireguard_addresses:VAR#"
      SERVER_CITIES: Brussels
      DOT_PROVIDERS: quad9

  # =========================
  # =         NGINX         =
  # =========================
  nginx-hetvleermuizenkind:
    <<: *defaults
    image: docker.io/library/nginx:latest
    container_name: nginx-hetvleermuizenkind
    networks:
      - traefik
    volumes:
      - "#VAR:general_path_appdata:VAR#/nginx/hetvleermuizenkind:/usr/share/nginx/html"
    environment:
      TZ: "#VAR:general_timezone:VAR#"

  # =========================
  # =     ERROR LOGGING     =
  # =========================
  # Go to GlitchTip url /register to create first user
  glitchtip:
    <<: *defaults
    image: docker.io/glitchtip/glitchtip:latest
    container_name: glitchtip
    hostname: "glitchtip.#VAR:general_domain_default:VAR#" # Prevents unvalidated sender in Mailjet
    depends_on:
      glitchtip-db:
        condition: service_healthy
      glitchtip-redis:
        condition: service_started
    volumes:
      - "#VAR:general_path_appdata:VAR#/glitchtip/uploads:/code/uploads"
    networks:
      - traefik
      - glitchtip
    environment:
      <<: *glitchtip-env

  glitchtip-worker:
    <<: *defaults
    image: docker.io/glitchtip/glitchtip:latest
    container_name: glitchtip-worker
    command: ./bin/run-celery-with-beat.sh
    hostname: "glitchtip.#VAR:general_domain_default:VAR#" # Prevents unvalidated sender in Mailjet
    depends_on:
      glitchtip-db:
        condition: service_healthy
      glitchtip-redis:
        condition: service_started
    networks:
      - glitchtip
    volumes:
      - "#VAR:general_path_appdata:VAR#/glitchtip/uploads:/code/uploads"
    environment:
      <<: *glitchtip-env
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: 2G

  glitchtip-migrate:
    <<: *defaults
    image: docker.io/glitchtip/glitchtip:latest
    container_name: glitchtip-migrate
    hostname: "glitchtip.#VAR:general_domain_default:VAR#" # Prevents unvalidated sender in Mailjet
    restart: "no" # Only run on "docker compose up"
    depends_on:
      glitchtip-db:
        condition: service_healthy
      glitchtip-redis:
        condition: service_started
    command: "./manage.py migrate"
    networks:
      - glitchtip
    environment:
      <<: *glitchtip-env

  glitchtip-db:
    <<: [*defaults, *healthcheck-postgres]
    # GlitchTip requires Postgres 12+
    # https://glitchtip.com/documentation/install
    image: docker.io/library/postgres:13-alpine
    container_name: glitchtip-db
    networks:
      - glitchtip
    volumes:
      - "#VAR:general_path_appdata:VAR#/glitchtip/db:/var/lib/postgresql/data"
      - "glitchtip-db-dump:/backup"
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      POSTGRES_DB: "glitchtip"
      POSTGRES_USER: "glitchtip"
      POSTGRES_PASSWORD: "#VAR:app_glitchtip_db_pass:VAR#"

  glitchtip-redis:
    <<: *defaults
    image: docker.io/library/redis:latest
    container_name: glitchtip-redis
    networks:
      - glitchtip
    environment:
      TZ: "#VAR:general_timezone:VAR#"

  # =========================
  # =       PDF tools       =
  # =========================
  stirling-pdf:
    <<: *defaults
    image: docker.io/frooodle/s-pdf:latest
    container_name: stirling-pdf
    networks:
      - traefik
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      DOCKER_ENABLE_SECURITY: "false"

  # =========================
  # =       MONITORING      =
  # =========================
  graylog:
    <<: *defaults
    image: docker.io/graylog/graylog:6.0
    container_name: graylog
    entrypoint: /usr/bin/tini -- wait-for-it graylog-elasticsearch:9200 --  /docker-entrypoint.sh
    volumes:
      # WARNING: ElasticSearch expects the data folder been owned by 1100:1100!
      - "#VAR:general_path_appdata:VAR#/graylog/graylog/config:/usr/share/graylog/config"
      - "#VAR:general_path_appdata:VAR#/graylog/graylog/journal:/usr/share/graylog/journal"
    networks:
      - traefik
      - graylog
    ports:
      - 1514:1514 # Syslog TCP
      - 1514:1514/udp # Syslog UDP
      - 5044:5044/tcp # Beats
      - 12201:12201 # GELF TCP
      - 12201:12201/udp # GELF UDP
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      GRAYLOG_PASSWORD_SECRET: "#VAR:app_graylog_secret:VAR#"
      GRAYLOG_ROOT_PASSWORD_SHA2: "#VAR:app_graylog_admin_password | hash('sha256'):VAR#"
      GRAYLOG_HTTP_EXTERNAL_URI: "https://logs.#VAR:general_domain_default:VAR#/"
      GRAYLOG_ELASTICSEARCH_HOSTS: http://graylog-elasticsearch:9200
      GRAYLOG_MONGODB_URI: mongodb://graylog-mongodb/graylog
      GRAYLOG_TRANSPORT_EMAIL_ENABLED: "true"
      GRAYLOG_TRANSPORT_EMAIL_HOSTNAME: "#VAR:mailjet_host:VAR#"
      GRAYLOG_TRANSPORT_EMAIL_PORT: "#VAR:mailjet_port_ssl:VAR#"
      GRAYLOG_TRANSPORT_EMAIL_USE_SSL: "true"
      GRAYLOG_TRANSPORT_EMAIL_USE_TLS: "false"
      GRAYLOG_TRANSPORT_EMAIL_USE_AUTH: "true"
      GRAYLOG_TRANSPORT_EMAIL_AUTH_USERNAME: "#VAR:mailjet_username:VAR#"
      GRAYLOG_TRANSPORT_EMAIL_AUTH_PASSWORD: "#VAR:mailjet_password:VAR#"
      GRAYLOG_TRANSPORT_EMAIL_FROM_EMAIL: graylog@jensw.be
      GRAYLOG_TRANSPORT_EMAIL_WEB_INTERFACE_URL: "https://logs.#VAR:general_domain_default:VAR#"
    depends_on:
      graylog-mongodb:
        condition: service_healthy
      graylog-elasticsearch:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 4G

  graylog-elasticsearch:
    <<: [*defaults, *healthcheck-elasticsearch]
    # "We caution you not to install or upgrade to Elasticsearch 7.11 and later! It is not supported."
    # See https://go2docs.graylog.org/current/setting_up_graylog/elasticsearch.htm
    image: docker.elastic.co/elasticsearch/elasticsearch-oss:7.10.2
    container_name: graylog-elasticsearch
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      http.host: "0.0.0.0"
      transport.host: localhost
      network.host: "0.0.0.0"
      ES_JAVA_OPTS: -Dlog4j2.formatMsgNoLookups=true -Xms512m -Xmx512m
    ulimits:
      memlock:
        soft: -1
        hard: -1
    networks:
      - graylog
    volumes:
      # WARNING: ElasticSearch expects the data folder been owned by 1000:1000!
      - "#VAR:general_path_appdata:VAR#/graylog/elasticsearch/data:/usr/share/elasticsearch/data"
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 4G

  graylog-mongodb:
    <<: [*defaults, *healthcheck-mongo]
    # MongoDB 6.0 is used at
    # https://go2docs.graylog.org/current/downloading_and_installing_graylog/docker_installation.htm
    image: docker.io/library/mongo:6.0
    container_name: graylog-mongodb
    environment:
      TZ: "#VAR:general_timezone:VAR#"
    networks:
      - graylog
    volumes:
      - "#VAR:general_path_appdata:VAR#/graylog/mongodb/data:/data/db"
      - graylog-mongodb-dump:/backup

  zabbix-web:
    <<: *defaults
    image: "docker.io/zabbix/zabbix-web-nginx-pgsql:alpine-#VAR:zabbix.server.version:VAR#-latest"
    container_name: zabbix-web
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
    depends_on:
      zabbix-server:
        condition: service_started
      zabbix-db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    networks:
      - traefik
      - zabbix
    stop_grace_period: 10s
    sysctls:
      - net.core.somaxconn=65535
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      DB_SERVER_HOST: zabbix-db
      POSTGRES_DB: zabbix
      POSTGRES_USER: zabbix
      POSTGRES_PASSWORD: "#VAR:app_zabbix_db_pass:VAR#"
      ZBX_SERVER_HOST: zabbix-server
      ZBX_SERVER_NAME: Kubo
    labels:
      - "com.zabbix.description=Zabbix frontend on Nginx web-server with PostgreSQL database support"
      - "com.zabbix.company=Zabbix LLC"
      - "com.zabbix.component=zabbix-frontend"
      - "com.zabbix.webserver=nginx"
      - "com.zabbix.dbtype=pgsql"
      - "com.zabbix.os=alpine"
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: 2G
        reservations:
          cpus: "0.5"
          memory: 256M

  zabbix-server:
    <<: [*defaults, *extra-hosts-docker-host]
    image: "docker.io/zabbix/zabbix-server-pgsql:alpine-#VAR:zabbix.server.version:VAR#-latest"
    container_name: zabbix-server
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      DB_SERVER_HOST: zabbix-db
      POSTGRES_DB: zabbix
      POSTGRES_USER: zabbix
      POSTGRES_PASSWORD: "#VAR:app_zabbix_db_pass:VAR#"
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
    ulimits:
      nproc: 65535
      nofile:
        soft: 20000
        hard: 40000
    depends_on:
      zabbix-db:
        condition: service_healthy
    networks:
      - zabbix
    stop_grace_period: 30s
    sysctls:
      - net.ipv4.ip_local_port_range=1024 64999
      - net.ipv4.conf.all.accept_redirects=0
      - net.ipv4.conf.all.secure_redirects=0
      - net.ipv4.conf.all.send_redirects=0
    labels:
      - "com.zabbix.description=Zabbix server with PostgreSQL database support"
      - "com.zabbix.company=Zabbix LLC"
      - "com.zabbix.component=zabbix-server"
      - "com.zabbix.dbtype=pgsql"
      - "com.zabbix.os=alpine"
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: 2G
        reservations:
          cpus: "0.5"
          memory: 512M

  zabbix-db:
    <<: [*defaults, *healthcheck-postgres]
    # https://www.zabbix.com/documentation/current/en/manual/installation/requirements
    image: docker.io/library/postgres:15-alpine
    container_name: zabbix-db
    stop_grace_period: 1m
    networks:
      - zabbix
    volumes:
      - "#VAR:general_path_appdata:VAR#/zabbix/db:/var/lib/postgresql/data"
      - "zabbix-db-dump:/backup"
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      POSTGRES_DB: "zabbix"
      POSTGRES_USER: "zabbix"
      POSTGRES_PASSWORD: "#VAR:app_zabbix_db_pass:VAR#"

  prometheus:
    <<: [*defaults, *extra-hosts-wireguard]
    image: quay.io/prometheus/prometheus:latest
    container_name: prometheus
    user: "#VAR:ansible_real_user_id:VAR#:#VAR:ansible_real_group_id:VAR#"
    networks:
      - traefik
      - prometheus
      - crowdsec
    environment:
      TZ: "#VAR:general_timezone:VAR#"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - "#VAR:general_path_appdata:VAR#/prometheus/data:/prometheus"
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 4G

  netperf-blackbox-exporter:
    <<: *defaults
    image: quay.io/prometheus/blackbox-exporter:latest
    container_name: netperf-blackbox-exporter
    command: --config.file=/config.yml
    environment:
      TZ: "#VAR:general_timezone:VAR#"
    volumes:
      - ./netperf/prometheus-blackbox-exporter.yml:/config.yml
    networks:
      - prometheus

  netperf-file-generator:
    <<: *defaults
    build: ./netperf/file-generator
    container_name: netperf-file-generator
    environment:
      TZ: "#VAR:general_timezone:VAR#"
    ports:
      - "#VAR:app_wireguard[0].ip_addresses[0]:VAR#:9116:8080"
    healthcheck:
      test: ["CMD", "/network-performance-file-generator", "--healthcheck"]

  grafana:
    <<: *defaults
    image: docker.io/grafana/grafana-oss
    container_name: grafana
    user: "#VAR:ansible_real_user_id:VAR#:#VAR:ansible_real_group_id:VAR#"
    networks:
      - traefik
      - prometheus
    volumes:
      - ./grafana:/provisioning
      - "#VAR:general_path_appdata:VAR#/grafana/data:/var/lib/grafana"
    environment:
      GF_PATHS_PROVISIONING: "/provisioning"
      GF_SERVER_ROOT_URL: "https://grafana.#VAR:general_domain_default:VAR#"
      GF_AUTH_ANONYMOUS_ENABLED: "false"
      GF_AUTH_BASIC_ENABLED: "false"
      GF_AUTH_DISABLE_LOGIN_FORM: "true"
      GF_AUTH_AUTO_LOGIN: "true"
      GF_AUTH_GENERIC_OAUTH_ENABLED: "true"
      GF_AUTH_GENERIC_OAUTH_NAME: Keycloak-OAuth
      GF_AUTH_GENERIC_OAUTH_ALLOW_SIGN_UP: "true"
      GF_AUTH_GENERIC_OAUTH_CLIENT_ID: "#VAR:app_grafana_oauth2_client_id:VAR#"
      GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET: "#VAR:app_grafana_oauth2_client_secret:VAR#"
      GF_AUTH_GENERIC_OAUTH_SCOPES: openid email profile offline_access roles
      GF_AUTH_GENERIC_OAUTH_EMAIL_ATTRIBUTE_PATH: email
      GF_AUTH_GENERIC_OAUTH_LOGIN_ATTRIBUTE_PATH: username
      GF_AUTH_GENERIC_OAUTH_NAME_ATTRIBUTE_PATH: full_name
      GF_AUTH_GENERIC_OAUTH_AUTH_URL: "#VAR:app_grafana_oauth2_base_url:VAR#/protocol/openid-connect/auth"
      GF_AUTH_GENERIC_OAUTH_TOKEN_URL: "#VAR:app_grafana_oauth2_base_url:VAR#/protocol/openid-connect/token"
      GF_AUTH_GENERIC_OAUTH_API_URL: "#VAR:app_grafana_oauth2_base_url:VAR#/protocol/openid-connect/userinfo"
      GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH: "contains(roles[*], 'admin') && 'Admin' || contains(roles[*], 'editor') && 'Editor' || 'Viewer'"

  # =========================
  # =        CROWDSEC       =
  # =========================
  crowdsec:
    <<: *defaults
    image: ghcr.io/crowdsecurity/crowdsec:v1.6.2-debian
    container_name: crowdsec
    ports:
      - 127.0.0.1:9000:8080
    networks:
      - crowdsec
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      COLLECTIONS: >-
        crowdsecurity/http-cve
        crowdsecurity/linux
        crowdsecurity/traefik
      PARSERS: >-
        crowdsecurity/nextcloud-whitelist
      GID: "#VAR:ansible_real_group_id:VAR#"
      BOUNCER_KEY_firewall: "#VAR:app_crowdsec_bouncer_firewall_key:VAR#"
    volumes:
      # Config
      - ./crowdsec/acquis.yaml:/etc/crowdsec/acquis.yaml
      - crowdsec-db:/var/lib/crowdsec/data/
      - crowdsec-config:/etc/crowdsec/
      # Logs
      - /var/log/journal:/var/log/host:ro # SSHD logs for Debian
      - traefik-access-logs:/var/log/traefik:ro

  # =========================
  # =         BACKUP        =
  # =========================
  github-backup:
    <<: *defaults
    image: docker.io/jenswbe/github-backup:latest
    container_name: github-backup
    restart: "no" # Will be called by systemd timer
    volumes:
      - ./github-backup/config.yml:/config.yml
      - github-backup:/backup
    environment:
      - "TZ=#VAR:general_timezone:VAR#"

  borgmatic:
    <<: *defaults
    image: ghcr.io/borgmatic-collective/borgmatic:1.8.9
    container_name: borgmatic
    privileged: true # Required for borg mount
    cap_add:
      - SYS_ADMIN # Required for borg mount
    volumes:
      # Backup locations
      - "github-backup:/mnt/source/github-backup/backup:ro"
      - "glitchtip-db-dump:/mnt/source/glitchtip/dbdump:ro"
      - "graylog-mongodb-dump:/mnt/source/graylog/mongodb:ro"
      - "#VAR:general_path_appdata:VAR#/home-automation/home-assistant/config:/mnt/source/home-automation/home-assistant/config:ro"
      - "miniflux-db-dump:/mnt/source/miniflux/dbdump:ro"
      - "nc-db-dump:/mnt/source/nextcloud/dbdump:ro"
      - "#VAR:general_path_appdata:VAR#/nextcloud/nextcloud/data:/mnt/source/nextcloud/data:ro"
      - "#VAR:general_path_appdata:VAR#/nextcloud/nextcloud/config:/mnt/source/nextcloud/config:ro"
      - "zabbix-db-dump:/mnt/source/zabbix/dbdump:ro"
      - "/data/important/Photos:/mnt/source/plex/photos:ro"
      - "/data/bulk/media/Music:/mnt/source/plex/music:ro"
      # Config and cache
      - "./borgmatic/borgmatic.d/config.yaml:/etc/borgmatic.d/config.yaml"
      - "./borgmatic/ssh/BorgHost:/root/.ssh/BorgHost"
      - "./borgmatic/ssh/known_hosts:/root/.ssh/known_hosts"
      - "#VAR:general_path_appdata:VAR#/borgmatic/borgmatic/config:/root/.config/borg"
      - "#VAR:general_path_appdata:VAR#/borgmatic/borgmatic/cache:/root/.cache/borg"
      - "#VAR:general_path_appdata:VAR#/borgmatic/borgmatic/restore:/mnt/restore"
    devices:
      - "/dev/fuse:/dev/fuse" # Required for borg mount
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      BACKUP_CRON: "0 3 * * *"
      BORG_PASSPHRASE: "#VAR:app_borgmatic_borg_passphrase:VAR#"
      BORG_HOSTNAME_IS_UNIQUE: "yes" # Automatically removes stale locks
      BORG_HOST_ID: "#VAR:ansible_hostname:VAR#"
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 4G

#################################################################
#                            NETWORKS                           #
#################################################################
networks:
  crowdsec:
    name: crowdsec

  glitchtip:
    name: glitchtip

  graylog:
    name: graylog

  home-automation:
    name: home-automation

  miniflux:
    name: miniflux

  nextcloud:
    name: nextcloud

  prometheus:
    name: prometheus

  traefik:
    name: traefik

  zabbix:
    name: zabbix

#################################################################
#                            VOLUMES                            #
#################################################################
volumes:
  crowdsec-config:
  crowdsec-db:
  github-backup:
  glitchtip-db-dump:
  graylog-mongodb-dump:
  ha-mosquitto-data:
  miniflux-db-dump:
  nc-db-dump:
  oauth2-proxy-helper-config:
  oauth2-proxy-helper-data:
  scrutiny-influxdb:
  scrutiny-sqlite:
  traefik-access-logs:
  traefik-cert:
  zabbix-db-dump:
