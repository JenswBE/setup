#VAR:ansible_managed | comment:VAR#

#################################################################
#                            DEFAULTS                           #
#################################################################

x-defaults: &defaults
  x-dummy: ""
  # Putting the anchor in this file ensures it's a valid YAML file for Renovate Bot
  #VAR:lookup('ansible.builtin.file', 'files/docker-compose-defaults.yml',) | indent(width=2):VAR#

x-extra-hosts-docker-host: &extra-hosts-docker-host
  extra_hosts:
    - "host.docker.internal:host-gateway"

x-healthcheck-elasticsearch: &healthcheck-elasticsearch
  healthcheck:
    test:
      [
        "CMD-SHELL",
        "curl --silent --fail localhost:9200/_cluster/health || exit 1",
      ]
    interval: 30s
    timeout: 10s
    retries: 5

x-healthcheck-mongo: &healthcheck-mongo
  healthcheck:
    test:
      [
        "CMD-SHELL",
        "mongosh --quiet localhost --eval 'quit(db.runCommand({ ping: 1 }).ok ? 0 : 1)'",
      ]
    interval: 30s
    timeout: 10s
    retries: 5

x-healthcheck-postgres: &healthcheck-postgres
  healthcheck:
    test:
      [
        "CMD-SHELL",
        "sh -c 'pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}'",
      ]
    interval: 30s
    timeout: 10s
    retries: 5

x-fix-prettier-formatting:

#################################################################
#                            SERVICES                           #
#################################################################
services:
  # =========================
  # =         PROXY         =
  # =========================
  # Having 2 reverse proxies (1 on host network and 1 on bridge network)
  # allows to have the real client IP's available inside the bridged
  # Caddy instance (using PROXY protocol). In case you would only have
  # a single reverse proxy on the host network, you would loose all the convenience
  # of using Docker networks.
  nginx:
    <<: *defaults
    image: docker.io/library/nginx:alpine
    container_name: nginx
    network_mode: host
    volumes:
      - ./nginx/default.conf:/etc/nginx/nginx.conf:ro

  caddy-public:
    <<: *defaults
    build: ./caddy
    container_name: caddy-public
    ports:
      - 127.0.0.1:2080:80
      - 127.0.0.1:2443:443
      - 127.0.0.1:28883:8883
    networks:
      - caddy-public
    volumes:
      - ./caddy/public/Caddyfile:/etc/caddy/Caddyfile:ro
      - ./caddy/public/configs:/etc/caddy/configs:ro
      - caddy-public-access-logs:/access_logs
      - caddy-public-config:/config
      - caddy-public-data:/data
    environment:
      TZ: "#VAR:general_timezone:VAR#"

  caddy-private:
    <<: *defaults
    build: ./caddy
    container_name: caddy-private
    networks:
      caddy-private-ip:
        ipv4_address: "#VAR:hostnames.private.kubo.ipv4:VAR#"
      caddy-private:
    volumes:
      - ./caddy/private/Caddyfile:/etc/caddy/Caddyfile:ro
      - ./caddy/private/configs:/etc/caddy/configs:ro
      - caddy-private-config:/config
      - caddy-private-data:/data
    environment:
      TZ: "#VAR:general_timezone:VAR#"

  # =========================
  # =    HOME AUTOMATION    =
  # =========================
  ha:
    <<: *defaults
    image: ghcr.io/home-assistant/home-assistant:stable
    container_name: ha
    networks:
      - caddy-private
      - home-automation
    volumes:
      - "./home-automation/home-assistant/automations.yaml:/config/automations.yaml"
      - "./home-automation/home-assistant/configuration.yaml:/config/configuration.yaml"
      - "#VAR:general_path_appdata:VAR#/home-automation/home-assistant/config:/config"
    environment:
      TZ: "#VAR:general_timezone:VAR#"

  ha-zigbee2mqtt:
    <<: *defaults
    image: ghcr.io/koenkk/zigbee2mqtt:latest
    container_name: ha-zigbee2mqtt
    networks:
      - home-automation
    depends_on:
      ha-mosquitto:
        condition: service_started
    volumes:
      - "#VAR:general_path_appdata:VAR#/home-automation/zigbee2mqtt:/app/data"
      - "./home-automation/zigbee2mqtt/configuration.yaml:/app/data/configuration.yaml"
    devices:
      - "/dev/serial/by-id/usb-Silicon_Labs_slae.sh_cc2652rb_stick_-_slaesh_s_iot_stuff_00_12_4B_00_22_98_86_5C-if00-port0:/dev/zigbee"
    environment:
      TZ: "#VAR:general_timezone:VAR#"

  ha-mosquitto:
    <<: *defaults
    image: docker.io/library/eclipse-mosquitto:latest
    container_name: ha-mosquitto
    networks:
      - caddy-private
      - home-automation
    volumes:
      - "./home-automation/mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf:ro"
      - "./home-automation/mosquitto/password_file:/mosquitto/config/password_file"
      - "ha-mosquitto-data:/mosquitto/data/"
    environment:
      TZ: "#VAR:general_timezone:VAR#"

  # =========================
  # =       MULTIMEDIA      =
  # =========================
  plex:
    <<: *defaults
    image: docker.io/plexinc/pms-docker:latest
    container_name: plex
    network_mode: host
    volumes:
      - "/data/bulk/media:/data/media"
      - "/data/important/Photos:/data/Photos"
      - "#VAR:general_path_appdata:VAR#/plex/config:/config"
      - "#VAR:general_path_appdata:VAR#/plex/transcode:/transcode"
    # devices: # for QuickSync support
    #   - "/dev/dri/card0:/dev/dri/card0"
    #   - "/dev/dri/renderD128:/dev/dri/renderD128"
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      PLEX_UID: "#VAR:ansible_real_user_id:VAR#"
      PLEX_GID: "#VAR:ansible_real_group_id:VAR#"
    deploy:
      resources:
        limits:
          cpus: "7"
          memory: 4G

  jellyfin:
    <<: *defaults
    image: docker.io/jellyfin/jellyfin:latest
    container_name: jellyfin
    user: "#VAR:ansible_real_user_id:VAR#:#VAR:ansible_real_group_id:VAR#"
    # See https://jellyfin.org/docs/general/administration/hardware-acceleration/intel#configure-with-linux-virtualization
    group_add: # By id as these may not exist within the container. Needed to provide permissions to the QSV/VA-API Devices
      - "105" # Group "render"
      - "44" # Group "video"
    devices:
      - /dev/dri/renderD128:/dev/dri/renderD128 # Intel Quick Sync Video (QSV)
    networks:
      - caddy-public
    ports:
      - "8096:8096"
    volumes:
      - "/data/bulk/media:/media"
      - "/data/important/Photos:/Photos"
      - "#VAR:general_path_appdata:VAR#/jellyfin/config:/config"
      - "#VAR:general_path_appdata:VAR#/jellyfin/cache:/cache"
    deploy:
      resources:
        limits:
          cpus: "7"
          memory: 4G

  # =========================
  # =   TINY MEDIA MANAGER  =
  # =========================
  tmm:
    <<: *defaults
    image: docker.io/tinymediamanager/tinymediamanager:latest
    container_name: tmm
    networks:
      - caddy-private
    volumes:
      - "/data/bulk/media:/media"
      - "#VAR:general_path_appdata:VAR#/tmm/data:/data"
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      USER_ID: "#VAR:ansible_real_user_id:VAR#"
      GROUP_ID: "#VAR:ansible_real_group_id:VAR#"
      LC_TIME: C.UTF-8
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: 2G

  # =========================
  # =       SYNCTHING       =
  # =========================
  syncthing:
    <<: *defaults
    image: docker.io/syncthing/syncthing:1
    container_name: syncthing
    hostname: "#VAR:ansible_hostname:VAR#"
    networks:
      - caddy-private
    ports:
      - "22000:22000"
      - "22000:22000/udp"
    volumes:
      - "#VAR:general_path_appdata:VAR#/syncthing/config:/var/syncthing"
      - "/data/bulk/media/Music:/data/music"
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      PUID: "#VAR:ansible_real_user_id:VAR#"
      PGID: "#VAR:ansible_real_user_id:VAR#"

  # =========================
  # =         SAMBA         =
  # =========================
  samba:
    <<: *defaults
    image: docker.io/dperson/samba:latest
    container_name: samba
    ports:
      - "445:445"
    volumes:
      - "/data/bulk/ps2-roms:/data/ps2"
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      USERID: "#VAR:ansible_real_user_id:VAR#"
      GROUPID: "#VAR:ansible_real_group_id:VAR#"
    command: >
      samba.sh -S
      -s "ps2;/data/ps2;yes;no;yes"

  # =========================
  # =       NEXTCLOUD       =
  # =========================
  nextcloud:
    <<: *defaults
    build: ./nextcloud
    container_name: nextcloud
    hostname: nextcloud
    networks:
      - caddy-public
      - nextcloud
    depends_on:
      nc-db:
        condition: service_healthy
    volumes:
      - "#VAR:general_path_appdata:VAR#/nextcloud/nextcloud/config:/var/www/html"
      - "#VAR:general_path_appdata:VAR#/nextcloud/nextcloud/data:/var/www/html/data"
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      MYSQL_DATABASE: "nextcloud"
      MYSQL_USER: "nextcloud"
      MYSQL_PASSWORD: "#VAR:app_nextcloud_db_pass_nextcloud:VAR#"
      MYSQL_HOST: "nc-db"
    deploy:
      resources:
        limits:
          cpus: "7"
          memory: 4G

  nc-db:
    <<: *defaults
    # Nextcloud currently only supports up to 10.6.
    # See https://docs.nextcloud.com/server/29/admin_manual/installation/system_requirements.html
    image: docker.io/library/mariadb:10.6
    command: --transaction-isolation=READ-COMMITTED --log-bin=ROW --innodb-read-only-compressed=OFF
    container_name: nc-db
    networks:
      - nextcloud
    volumes:
      - "#VAR:general_path_appdata:VAR#/nextcloud/mariadb/data:/var/lib/mysql"
      - nc-db-dump:/backup
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      MARIADB_ROOT_PASSWORD: "#VAR:app_nextcloud_db_pass_root:VAR#"
      MARIADB_PASSWORD: "#VAR:app_nextcloud_db_pass_nextcloud:VAR#"
      MARIADB_DATABASE: "nextcloud"
      MARIADB_USER: "nextcloud"
      MARIADB_AUTO_UPGRADE: "1"
    healthcheck: # Based on https://mariadb.com/kb/en/using-healthcheck-sh/
      test: ["CMD", "healthcheck.sh", "--connect", "--innodb_initialized"]
      start_period: 10s
      interval: 10s
      timeout: 5s
      retries: 3

  # =========================
  # =        IMMICH         =
  # =========================
  immich: # Set ML url in settings to http://immich-ml:3003
    <<: *defaults
    image: ghcr.io/immich-app/immich-server:v1.131.3
    container_name: immich
    networks:
      - caddy-public
      - immich
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      DB_HOSTNAME: immich-db
      DB_PASSWORD: "#VAR:app_immich_db_pass_postgres:VAR#"
      REDIS_HOSTNAME: immich-redis
      IMMICH_TELEMETRY_INCLUDE: all
    volumes:
      - "#VAR:general_path_appdata:VAR#/immich/immich/upload:/usr/src/app/upload"
      - /etc/localtime:/etc/localtime:ro
    devices:
      - /dev/dri:/dev/dri
    depends_on:
      immich-redis:
        condition: service_healthy
      immich-db:
        condition: service_healthy
    healthcheck:
      disable: false
    ports:
      - 2283:2283
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 2G

  immich-ml:
    <<: *defaults
    image: ghcr.io/immich-app/immich-machine-learning:v1.131.2-openvino
    container_name: immich-ml
    networks:
      - immich
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      DB_HOSTNAME: immich-db
      DB_PASSWORD: "#VAR:app_immich_db_pass_postgres:VAR#"
      REDIS_HOSTNAME: immich-redis
    volumes:
      - immich-model-cache:/cache
      - /dev/bus/usb:/dev/bus/usb # For hardware-accelerated machine learning
    device_cgroup_rules: # For hardware-accelerated machine learning
      - "c 189:* rmw"
    devices:
      - /dev/dri:/dev/dri
    healthcheck:
      disable: false
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 4G

  immich-redis:
    <<: *defaults
    image: docker.io/redis:6.2-alpine@sha256:148bb5411c184abd288d9aaed139c98123eeb8824c5d3fce03cf721db58066d8
    container_name: immich-redis
    networks:
      - immich
    environment:
      TZ: "#VAR:general_timezone:VAR#"
    healthcheck:
      test: redis-cli ping || exit 1

  immich-db:
    <<: *defaults
    image: docker.io/tensorchord/pgvecto-rs:pg14-v0.2.0@sha256:739cdd626151ff1f796dc95a6591b55a714f341c737e27f045019ceabf8e8c52
    container_name: immich-db
    networks:
      - immich
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      POSTGRES_DB: immich
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: "#VAR:app_immich_db_pass_postgres:VAR#"
      POSTGRES_INITDB_ARGS: "--data-checksums"
    volumes:
      - "#VAR:general_path_appdata:VAR#/immich/postgresql/data:/var/lib/postgresql/data"
      - "immich-db-dump:/backup"
    healthcheck:
      test: >-
        pg_isready --dbname="$${POSTGRES_DB}" --username="$${POSTGRES_USER}" || exit 1;
        Chksum="$$(psql --dbname="$${POSTGRES_DB}" --username="$${POSTGRES_USER}" --tuples-only --no-align
        --command='SELECT COALESCE(SUM(checksum_failures), 0) FROM pg_stat_database')";
        echo "checksum failure count is $$Chksum";
        [ "$$Chksum" = '0' ] || exit 1
      interval: 5m
      start_interval: 30s
      start_period: 5m
    command: >-
      postgres
      -c shared_preload_libraries=vectors.so
      -c 'search_path="$$user", public, vectors'
      -c logging_collector=on
      -c max_wal_size=2GB
      -c shared_buffers=512MB
      -c wal_compression=on

  # =========================
  # =        TORRENT        =
  # =========================
  transmission:
    <<: *defaults
    image: docker.io/linuxserver/transmission:latest
    container_name: transmission
    network_mode: service:transmission-vpn
    volumes:
      - "#VAR:general_path_appdata:VAR#/transmission/config:/config"
      - "#VAR:general_path_appdata:VAR#/transmission/downloads:/downloads/incomplete"
      - "/data/bulk/media/Nazien:/downloads/complete"
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      PUID: "#VAR:ansible_real_user_id:VAR#"
      PGID: "#VAR:ansible_real_group_id:VAR#"

  transmission-vpn:
    <<: *defaults
    image: ghcr.io/qdm12/gluetun:latest
    container_name: transmission-vpn
    cap_add:
      - NET_ADMIN
    networks:
      - caddy-public
      - caddy-private
    devices:
      - /dev/net/tun:/dev/net/tun
    volumes:
      - "#VAR:general_path_appdata:VAR#/gluetun/config:/gluetun"
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      VPN_SERVICE_PROVIDER: mullvad
      VPN_TYPE: wireguard
      WIREGUARD_PRIVATE_KEY: "#VAR:app_transmission_wireguard_private_key:VAR#"
      WIREGUARD_ADDRESSES: "#VAR:app_transmission_wireguard_addresses:VAR#"
      SERVER_CITIES: Brussels
      DOT_PROVIDERS: quad9

  # ==========================
  # =      DNS - BLOCKY      =
  # ==========================
  blocky:
    <<: *defaults
    image: ghcr.io/0xerr0r/blocky:latest
    container_name: blocky
    user: "#VAR:ansible_real_user_id:VAR#:#VAR:ansible_real_group_id:VAR#"
    networks:
      - caddy-private
    ports:
      - "53:53/tcp" # DNS TCP
      - "53:53/udp" # DNS UDP
    volumes:
      - ./blocky/config.txt:/app/config.yml:ro

  # =========================
  # =       PDF tools       =
  # =========================
  stirling-pdf:
    <<: *defaults
    image: ghcr.io/stirling-tools/stirling-pdf:latest
    container_name: stirling-pdf
    networks:
      - caddy-private
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      DOCKER_ENABLE_SECURITY: "false"
    volumes:
      - "#VAR:general_path_appdata:VAR#/stirling-pdf/configs:/configs"

  # ===========================
  # =    UNIFI NETWORK APP    =
  # ===========================
  unifi-network-app:
    <<: *defaults
    image: lscr.io/linuxserver/unifi-network-application:latest
    container_name: unifi-network-app
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      PUID: "1000"
      PGID: "1000"
      MEM_LIMIT: "1024"
      MEM_STARTUP: "1024"
      MONGO_HOST: unifi-mongodb
      MONGO_PORT: "27017"
      MONGO_USER: "#VAR:app_unifi_mongodb_user:VAR#"
      MONGO_PASS: "#VAR:app_unifi_mongodb_password:VAR#"
      MONGO_DBNAME: unifi
    volumes:
      - "#VAR:general_path_appdata:VAR#/unifi/network-app/config:/config"
    ports:
      - "10001:10001/udp" # Required for AP discovery
      - "8080:8080" # Required for device communication
    networks:
      - caddy-private
      - unifi
    depends_on:
      unifi-mongodb:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: 2G

  unifi-mongodb: # NOTE: MongoDB 5.0+ requires AVX!
    <<: [*defaults, *healthcheck-mongo]
    # "Version 8.1 and newer supports up to MongoDB 7.0."
    # See release notes at https://www.ui.com/download/releases/network-server
    build: ./unifi/mongodb
    container_name: unifi-mongodb
    environment:
      TZ: "#VAR:general_timezone:VAR#"
    networks:
      - unifi
    volumes:
      - "#VAR:general_path_appdata:VAR#/unifi/mongodb/data:/data/db"
      - unifi-mongodb-dump:/backup
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: 2G

  # =========================
  # =       MONITORING      =
  # =========================
  graylog:
    <<: *defaults
    image: docker.io/graylog/graylog:6.1
    container_name: graylog
    entrypoint: /usr/bin/tini -- wait-for-it graylog-elasticsearch:9200 --  /docker-entrypoint.sh
    volumes:
      # WARNING: ElasticSearch expects the data folder been owned by 1100:1100!
      - "#VAR:general_path_appdata:VAR#/graylog/graylog/config:/usr/share/graylog/config"
      - "#VAR:general_path_appdata:VAR#/graylog/graylog/journal:/usr/share/graylog/journal"
    networks:
      - caddy-private
      - graylog
    ports:
      - 1514:1514 # Syslog TCP
      - 1514:1514/udp # Syslog UDP
      - 5044:5044/tcp # Beats
      - 12201:12201 # GELF TCP
      - 12201:12201/udp # GELF UDP
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      GRAYLOG_PASSWORD_SECRET: "#VAR:app_graylog_secret:VAR#"
      GRAYLOG_ROOT_PASSWORD_SHA2: "#VAR:app_graylog_admin_password | hash('sha256'):VAR#"
      GRAYLOG_HTTP_EXTERNAL_URI: "https://logs.#VAR:general_domain_local:VAR#/"
      GRAYLOG_ELASTICSEARCH_HOSTS: http://graylog-elasticsearch:9200
      GRAYLOG_MONGODB_URI: mongodb://graylog-mongodb/graylog
      GRAYLOG_TRANSPORT_EMAIL_ENABLED: "true"
      GRAYLOG_TRANSPORT_EMAIL_HOSTNAME: "#VAR:mailjet_host:VAR#"
      GRAYLOG_TRANSPORT_EMAIL_PORT: "#VAR:mailjet_port_ssl:VAR#"
      GRAYLOG_TRANSPORT_EMAIL_USE_SSL: "true"
      GRAYLOG_TRANSPORT_EMAIL_USE_TLS: "false"
      GRAYLOG_TRANSPORT_EMAIL_USE_AUTH: "true"
      GRAYLOG_TRANSPORT_EMAIL_AUTH_USERNAME: "#VAR:mailjet_username:VAR#"
      GRAYLOG_TRANSPORT_EMAIL_AUTH_PASSWORD: "#VAR:mailjet_password:VAR#"
      GRAYLOG_TRANSPORT_EMAIL_FROM_EMAIL: graylog@jensw.be
      GRAYLOG_TRANSPORT_EMAIL_WEB_INTERFACE_URL: "https://logs.#VAR:general_domain_local:VAR#"
    depends_on:
      graylog-mongodb:
        condition: service_healthy
      graylog-elasticsearch:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 4G

  graylog-elasticsearch:
    <<: [*defaults, *healthcheck-elasticsearch]
    # "We caution you not to install or upgrade to Elasticsearch 7.11 and later! It is not supported."
    # See https://go2docs.graylog.org/current/setting_up_graylog/elasticsearch.htm
    image: docker.elastic.co/elasticsearch/elasticsearch-oss:7.10.2
    container_name: graylog-elasticsearch
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      http.host: "0.0.0.0"
      transport.host: localhost
      network.host: "0.0.0.0"
      ES_JAVA_OPTS: -Dlog4j2.formatMsgNoLookups=true -Xms512m -Xmx512m
    ulimits:
      memlock:
        soft: -1
        hard: -1
    networks:
      - graylog
    volumes:
      # WARNING: ElasticSearch expects the data folder been owned by 1000:1000!
      - "#VAR:general_path_appdata:VAR#/graylog/elasticsearch/data:/usr/share/elasticsearch/data"
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 4G

  graylog-mongodb:
    <<: [*defaults, *healthcheck-mongo]
    # MongoDB 6.0 is used at
    # https://go2docs.graylog.org/current/downloading_and_installing_graylog/docker_installation.htm
    image: docker.io/library/mongo:6.0
    container_name: graylog-mongodb
    environment:
      TZ: "#VAR:general_timezone:VAR#"
    networks:
      - graylog
    volumes:
      - "#VAR:general_path_appdata:VAR#/graylog/mongodb/data:/data/db"
      - graylog-mongodb-dump:/backup

  zabbix-web:
    <<: *defaults
    image: "docker.io/zabbix/zabbix-web-nginx-pgsql:alpine-#VAR:zabbix.server.version:VAR#-latest"
    container_name: zabbix-web
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
    depends_on:
      zabbix-server:
        condition: service_started
      zabbix-db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    networks:
      - caddy-private
      - zabbix
    stop_grace_period: 10s
    sysctls:
      - net.core.somaxconn=65535
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      DB_SERVER_HOST: zabbix-db
      POSTGRES_DB: zabbix
      POSTGRES_USER: zabbix
      POSTGRES_PASSWORD: "#VAR:app_zabbix_db_pass:VAR#"
      ZBX_SERVER_HOST: zabbix-server
      ZBX_SERVER_NAME: Kubo
    labels:
      - "com.zabbix.description=Zabbix frontend on Nginx web-server with PostgreSQL database support"
      - "com.zabbix.company=Zabbix LLC"
      - "com.zabbix.component=zabbix-frontend"
      - "com.zabbix.webserver=nginx"
      - "com.zabbix.dbtype=pgsql"
      - "com.zabbix.os=alpine"
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: 2G
        reservations:
          cpus: "0.5"
          memory: 256M

  zabbix-server:
    <<: [*defaults, *extra-hosts-docker-host]
    image: "docker.io/zabbix/zabbix-server-pgsql:alpine-#VAR:zabbix.server.version:VAR#-latest"
    container_name: zabbix-server
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      DB_SERVER_HOST: zabbix-db
      POSTGRES_DB: zabbix
      POSTGRES_USER: zabbix
      POSTGRES_PASSWORD: "#VAR:app_zabbix_db_pass:VAR#"
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
    ulimits:
      nproc: 65535
      nofile:
        soft: 20000
        hard: 40000
    depends_on:
      zabbix-db:
        condition: service_healthy
    networks:
      - zabbix
    stop_grace_period: 30s
    sysctls:
      - net.ipv4.ip_local_port_range=1024 64999
      - net.ipv4.conf.all.accept_redirects=0
      - net.ipv4.conf.all.secure_redirects=0
      - net.ipv4.conf.all.send_redirects=0
    labels:
      - "com.zabbix.description=Zabbix server with PostgreSQL database support"
      - "com.zabbix.company=Zabbix LLC"
      - "com.zabbix.component=zabbix-server"
      - "com.zabbix.dbtype=pgsql"
      - "com.zabbix.os=alpine"
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: 2G
        reservations:
          cpus: "0.5"
          memory: 512M

  zabbix-db:
    <<: [*defaults, *healthcheck-postgres]
    # https://www.zabbix.com/documentation/current/en/manual/installation/requirements
    image: docker.io/library/postgres:15-alpine
    container_name: zabbix-db
    stop_grace_period: 1m
    networks:
      - zabbix
    volumes:
      - "#VAR:general_path_appdata:VAR#/zabbix/db:/var/lib/postgresql/data"
      - "zabbix-db-dump:/backup"
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      POSTGRES_DB: "zabbix"
      POSTGRES_USER: "zabbix"
      POSTGRES_PASSWORD: "#VAR:app_zabbix_db_pass:VAR#"

  prometheus:
    <<: [*defaults]
    image: quay.io/prometheus/prometheus:latest
    container_name: prometheus
    user: "#VAR:ansible_real_user_id:VAR#:#VAR:ansible_real_group_id:VAR#"
    networks:
      - caddy-private
      - prometheus
      - crowdsec
    environment:
      TZ: "#VAR:general_timezone:VAR#"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - "#VAR:general_path_appdata:VAR#/prometheus/data:/prometheus"
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 4G

  netperf-blackbox-exporter:
    <<: *defaults
    image: quay.io/prometheus/blackbox-exporter:latest
    container_name: netperf-blackbox-exporter
    command: --config.file=/config.yml
    environment:
      TZ: "#VAR:general_timezone:VAR#"
    volumes:
      - ./netperf/prometheus-blackbox-exporter.yml:/config.yml
    networks:
      - prometheus

  netperf-file-generator:
    <<: *defaults
    build: ./netperf/file-generator
    container_name: netperf-file-generator
    environment:
      TZ: "#VAR:general_timezone:VAR#"
    ports:
      - "9116:8080"
    healthcheck:
      test: ["CMD", "/network-performance-file-generator", "--healthcheck"]

  grafana:
    <<: *defaults
    image: docker.io/grafana/grafana-oss
    container_name: grafana
    user: "#VAR:ansible_real_user_id:VAR#:#VAR:ansible_real_group_id:VAR#"
    networks:
      - caddy-private
      - prometheus
    volumes:
      - ./grafana:/provisioning
      - "#VAR:general_path_appdata:VAR#/grafana/data:/var/lib/grafana"
    environment:
      GF_PATHS_PROVISIONING: "/provisioning"
      GF_SERVER_ROOT_URL: "https://grafana.#VAR:general_domain_local:VAR#"
      GF_AUTH_ANONYMOUS_ENABLED: "false"
      GF_AUTH_BASIC_ENABLED: "false"
      GF_AUTH_DISABLE_LOGIN_FORM: "true"
      GF_AUTH_AUTO_LOGIN: "true"
      GF_AUTH_GENERIC_OAUTH_ENABLED: "true"
      GF_AUTH_GENERIC_OAUTH_NAME: Keycloak-OAuth
      GF_AUTH_GENERIC_OAUTH_ALLOW_SIGN_UP: "true"
      GF_AUTH_GENERIC_OAUTH_CLIENT_ID: "#VAR:app_grafana_oauth2_client_id:VAR#"
      GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET: "#VAR:app_grafana_oauth2_client_secret:VAR#"
      GF_AUTH_GENERIC_OAUTH_SCOPES: openid email profile offline_access roles
      GF_AUTH_GENERIC_OAUTH_EMAIL_ATTRIBUTE_PATH: email
      GF_AUTH_GENERIC_OAUTH_LOGIN_ATTRIBUTE_PATH: username
      GF_AUTH_GENERIC_OAUTH_NAME_ATTRIBUTE_PATH: full_name
      GF_AUTH_GENERIC_OAUTH_AUTH_URL: "#VAR:app_grafana_oauth2_base_url:VAR#/protocol/openid-connect/auth"
      GF_AUTH_GENERIC_OAUTH_TOKEN_URL: "#VAR:app_grafana_oauth2_base_url:VAR#/protocol/openid-connect/token"
      GF_AUTH_GENERIC_OAUTH_API_URL: "#VAR:app_grafana_oauth2_base_url:VAR#/protocol/openid-connect/userinfo"
      GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH: "contains(roles[*], 'admin') && 'Admin' || contains(roles[*], 'editor') && 'Editor' || 'Viewer'"

  # =========================
  # =        CROWDSEC       =
  # =========================
  crowdsec:
    <<: *defaults
    image: ghcr.io/crowdsecurity/crowdsec:v1.6.8-debian
    container_name: crowdsec
    ports:
      - 127.0.0.1:9000:8080
    networks:
      - caddy-private
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      COLLECTIONS: >-
        crowdsecurity/caddy
        crowdsecurity/http-cve
        crowdsecurity/linux
      PARSERS: >-
        crowdsecurity/nextcloud-whitelist
      GID: "#VAR:ansible_real_group_id:VAR#"
      BOUNCER_KEY_firewall: "#VAR:app_crowdsec_bouncer_firewall_key:VAR#"
    volumes:
      # Config
      - ./crowdsec/acquis.yaml:/etc/crowdsec/acquis.yaml
      - crowdsec-db:/var/lib/crowdsec/data/
      - crowdsec-config:/etc/crowdsec/
      # Logs
      - /var/log/journal:/var/log/host:ro # SSHD logs for Debian
      - caddy-public-access-logs:/var/log/caddy:ro

  # =========================
  # =         BACKUP        =
  # =========================
  github-backup:
    <<: *defaults
    image: docker.io/jenswbe/github-backup:latest
    container_name: github-backup
    restart: "no" # Will be called by systemd timer
    volumes:
      - ./github-backup/config.yml:/config.yml
      - github-backup:/backup
    environment:
      - "TZ=#VAR:general_timezone:VAR#"

  borgmatic:
    <<: *defaults
    image: ghcr.io/borgmatic-collective/borgmatic:1.9
    container_name: borgmatic
    cap_add:
      - SYS_ADMIN # Required for borg mount
    volumes:
      # Backup locations
      - "github-backup:/mnt/source/github-backup/backup:ro"
      - "graylog-mongodb-dump:/mnt/source/graylog/mongodb:ro"
      - "#VAR:general_path_appdata:VAR#/home-automation/home-assistant/config:/mnt/source/home-automation/home-assistant/config:ro"
      - "#VAR:general_path_appdata:VAR#/immich/immich/upload:/mnt/source/immich/upload"
      - "immich-db-dump:/mnt/source/immich/dbdump:ro"
      - "nc-db-dump:/mnt/source/nextcloud/dbdump:ro"
      - "#VAR:general_path_appdata:VAR#/nextcloud/nextcloud/data:/mnt/source/nextcloud/data:ro"
      - "#VAR:general_path_appdata:VAR#/nextcloud/nextcloud/config:/mnt/source/nextcloud/config:ro"
      - "unifi-mongodb-dump:/mnt/source/unifi/mongodb:ro"
      - "zabbix-db-dump:/mnt/source/zabbix/dbdump:ro"
      - "/data/important/Photos:/mnt/source/plex/photos:ro"
      - "/data/bulk/media/Music:/mnt/source/plex/music:ro"
      # Config and cache
      - "./borgmatic/borgmatic.d/config.yaml:/etc/borgmatic.d/config.yaml"
      - "./borgmatic/ssh/BorgHost:/root/.ssh/BorgHost"
      - "./borgmatic/ssh/known_hosts:/root/.ssh/known_hosts"
      - "#VAR:general_path_appdata:VAR#/borgmatic/borgmatic/config:/root/.config/borg"
      - "#VAR:general_path_appdata:VAR#/borgmatic/borgmatic/cache:/root/.cache/borg"
      - "#VAR:general_path_appdata:VAR#/borgmatic/borgmatic/restore:/mnt/restore"
    devices:
      - "/dev/fuse:/dev/fuse" # Required for borg mount
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      BACKUP_CRON: "0 3 * * *"
      BORG_PASSPHRASE: "#VAR:app_borgmatic_borg_passphrase:VAR#"
      BORG_HOSTNAME_IS_UNIQUE: "yes" # Automatically removes stale locks
      BORG_HOST_ID: "#VAR:ansible_hostname:VAR#"
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 4G

#################################################################
#                            NETWORKS                           #
#################################################################
networks:
  caddy-public:
    name: caddy-public

  caddy-private-ip:
    name: aaa-caddy-private-ip # Must be alphabetically before network "caddy-private" for Docker to use it as default gateway
    driver: macvlan
    driver_opts:
      parent: enp3s0
    ipam:
      config:
        - subnet: "#VAR: (hostnames.private.kubo.ipv4 + '/24') | ansible.utils.ipaddr('network/prefix') :VAR#"
          ip_range: "#VAR:hostnames.private.kubo.ipv4:VAR#/32"
          gateway: "#VAR: (hostnames.private.kubo.ipv4 + '/24') | ansible.utils.ipaddr('1') | ansible.utils.ipaddr('address') :VAR#"

  caddy-private:
    name: caddy-private

  crowdsec:
    name: crowdsec

  graylog:
    name: graylog

  home-automation:
    name: home-automation

  immich:
    name: immich

  nextcloud:
    name: nextcloud

  prometheus:
    name: prometheus

  unifi:
    name: unifi

  zabbix:
    name: zabbix

#################################################################
#                            VOLUMES                            #
#################################################################
volumes:
  caddy-private-config:
  caddy-private-data:
  caddy-public-access-logs:
  caddy-public-config:
  caddy-public-data:
  crowdsec-config:
  crowdsec-db:
  github-backup:
  graylog-mongodb-dump:
  ha-mosquitto-data:
  immich-model-cache:
  immich-db-dump:
  nc-db-dump:
  scrutiny-influxdb:
  scrutiny-sqlite:
  unifi-mongodb-dump:
  zabbix-db-dump:
