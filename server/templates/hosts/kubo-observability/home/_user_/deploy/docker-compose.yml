#VAR:ansible_managed | comment:VAR#

#################################################################
#                            DEFAULTS                           #
#################################################################

x-defaults: &defaults
  x-dummy: ""
  # Putting the anchor in this file ensures it's a valid YAML file for Renovate Bot
  #VAR:lookup('ansible.builtin.file', 'files/docker-compose-defaults.yml',) | indent(width=2):VAR#

x-extra-hosts-docker-host: &extra-hosts-docker-host
  extra_hosts:
    - "host.docker.internal:host-gateway"

x-healthcheck-elasticsearch: &healthcheck-elasticsearch
  healthcheck:
    test:
      [
        "CMD-SHELL",
        "curl --silent --fail localhost:9200/_cluster/health || exit 1",
      ]
    interval: 30s
    timeout: 10s
    retries: 5

x-healthcheck-mongo: &healthcheck-mongo
  healthcheck:
    test:
      [
        "CMD-SHELL",
        "mongosh --quiet localhost --eval 'quit(db.runCommand({ ping: 1 }).ok ? 0 : 1)'",
      ]
    interval: 30s
    timeout: 10s
    retries: 5

x-healthcheck-postgres: &healthcheck-postgres
  healthcheck:
    test:
      [
        "CMD-SHELL",
        "sh -c 'pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}'",
      ]
    interval: 30s
    timeout: 10s
    retries: 5

#################################################################
#                            SERVICES                           #
#################################################################
services:
  # =========================
  # =         PROXY         =
  # =========================
  # Having 2 reverse proxies (1 on host network and 1 on bridge network)
  # allows to have the real client IP's available inside the bridged
  # Caddy instance (using PROXY protocol). In case you would only have
  # a single reverse proxy on the host network, you would loose all the convenience
  # of using Docker networks.
  nginx:
    <<: *defaults
    image: docker.io/library/nginx:alpine
    container_name: nginx
    network_mode: host
    volumes:
      - ./nginx/default.conf:/etc/nginx/nginx.conf:ro

  caddy:
    <<: *defaults
    build: ./caddy
    container_name: caddy
    ports:
      - 127.0.0.1:2080:80
      - 127.0.0.1:2443:443
    networks:
      - caddy
    volumes:
      - ./caddy/Caddyfile:/etc/caddy/Caddyfile:ro
      - ./caddy/configs:/etc/caddy/configs:ro
      - caddy-access-logs:/access_logs
      - caddy-config:/config
      - caddy-data:/data
    environment:
      TZ: "#VAR:general_timezone:VAR#"

  # =========================
  # =         ZABBIX        =
  # =========================
  zabbix-web:
    <<: *defaults
    image: "docker.io/zabbix/zabbix-web-nginx-pgsql:alpine-#VAR:zabbix_server_version:VAR#-latest"
    container_name: zabbix-web
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
    depends_on:
      zabbix-server:
        condition: service_started
      zabbix-db:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 30s
    networks:
      - caddy
      - zabbix
    stop_grace_period: 10s
    sysctls:
      - net.core.somaxconn=65535
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      DB_SERVER_HOST: zabbix-db
      POSTGRES_DB: zabbix
      POSTGRES_USER: zabbix
      POSTGRES_PASSWORD: "#VAR:app_zabbix_db_pass:VAR#"
      ZBX_SERVER_HOST: zabbix-server
      ZBX_SERVER_NAME: Kubo
    labels:
      - "com.zabbix.description=Zabbix frontend on Nginx web-server with PostgreSQL database support"
      - "com.zabbix.company=Zabbix LLC"
      - "com.zabbix.component=zabbix-frontend"
      - "com.zabbix.webserver=nginx"
      - "com.zabbix.dbtype=pgsql"
      - "com.zabbix.os=alpine"
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: 2G
        reservations:
          cpus: "0.5"
          memory: 256M

  zabbix-server:
    <<: [*defaults, *extra-hosts-docker-host]
    image: "docker.io/zabbix/zabbix-server-pgsql:alpine-#VAR:zabbix_server_version:VAR#-latest"
    container_name: zabbix-server
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      DB_SERVER_HOST: zabbix-db
      POSTGRES_DB: zabbix
      POSTGRES_USER: zabbix
      POSTGRES_PASSWORD: "#VAR:app_zabbix_db_pass:VAR#"
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - /etc/timezone:/etc/timezone:ro
    ulimits:
      nproc: 65535
      nofile:
        soft: 20000
        hard: 40000
    depends_on:
      zabbix-db:
        condition: service_healthy
    networks:
      - zabbix
    stop_grace_period: 30s
    sysctls:
      - net.ipv4.ip_local_port_range=1024 64999
      - net.ipv4.conf.all.accept_redirects=0
      - net.ipv4.conf.all.secure_redirects=0
      - net.ipv4.conf.all.send_redirects=0
    labels:
      - "com.zabbix.description=Zabbix server with PostgreSQL database support"
      - "com.zabbix.company=Zabbix LLC"
      - "com.zabbix.component=zabbix-server"
      - "com.zabbix.dbtype=pgsql"
      - "com.zabbix.os=alpine"
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: 2G
        reservations:
          cpus: "0.5"
          memory: 512M

  zabbix-db:
    <<: [*defaults, *healthcheck-postgres]
    # https://www.zabbix.com/documentation/current/en/manual/installation/requirements
    image: docker.io/library/postgres:15-alpine
    container_name: zabbix-db
    stop_grace_period: 1m
    networks:
      - zabbix
    volumes:
      - "#VAR:general_path_appdata:VAR#/zabbix/db:/var/lib/postgresql/data"
      - "zabbix-db-dump:/backup"
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      POSTGRES_DB: "zabbix"
      POSTGRES_USER: "zabbix"
      POSTGRES_PASSWORD: "#VAR:app_zabbix_db_pass:VAR#"
    deploy:
      resources:
        limits:
          cpus: "2"
          memory: 1G

  # =========================
  # =       PROMETHEUS      =
  # =========================
  prometheus:
    <<: [*defaults]
    image: quay.io/prometheus/prometheus:latest
    container_name: prometheus
    user: "#VAR:ansible_real_user_id:VAR#:#VAR:ansible_real_group_id:VAR#"
    networks:
      - caddy
      - prometheus
    environment:
      TZ: "#VAR:general_timezone:VAR#"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - "#VAR:general_path_appdata:VAR#/prometheus/data:/prometheus"
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 4G

  # =========================
  # =        NETPERF        =
  # =========================
  netperf-blackbox-exporter:
    <<: *defaults
    image: quay.io/prometheus/blackbox-exporter:latest
    container_name: netperf-blackbox-exporter
    command: --config.file=/config.yml
    environment:
      TZ: "#VAR:general_timezone:VAR#"
    volumes:
      - ./netperf/prometheus-blackbox-exporter.yml:/config.yml
    networks:
      - prometheus

  netperf-file-generator:
    <<: *defaults
    build: ./netperf/file-generator
    container_name: netperf-file-generator
    environment:
      TZ: "#VAR:general_timezone:VAR#"
    ports:
      - "9116:8080"
    healthcheck:
      test: ["CMD", "/network-performance-file-generator", "--healthcheck"]

  # =========================
  # =        GRAFANA        =
  # =========================
  # Manual action: Set receiver for default contact point
  grafana:
    <<: *defaults
    image: docker.io/grafana/grafana-oss:latest
    container_name: grafana
    user: "#VAR:ansible_real_user_id:VAR#:#VAR:ansible_real_group_id:VAR#"
    networks:
      - caddy
      - loki
      - prometheus
    volumes:
      - ./grafana:/provisioning
      - "#VAR:general_path_appdata:VAR#/grafana/data:/var/lib/grafana"
    environment:
      TZ: "#VAR:general_timezone:VAR#"
      GF_PATHS_PROVISIONING: "/provisioning"
      GF_SERVER_ROOT_URL: "https://grafana.#VAR:general_domain_local:VAR#"
      GF_AUTH_ANONYMOUS_ENABLED: "false"
      GF_AUTH_BASIC_ENABLED: "false"
      GF_AUTH_DISABLE_LOGIN_FORM: "true"
      GF_AUTH_AUTO_LOGIN: "true"
      GF_AUTH_GENERIC_OAUTH_ENABLED: "true"
      GF_AUTH_GENERIC_OAUTH_NAME: Keycloak-OAuth
      GF_AUTH_GENERIC_OAUTH_ALLOW_SIGN_UP: "true"
      GF_AUTH_GENERIC_OAUTH_CLIENT_ID: "#VAR:app_grafana_oauth2_client_id:VAR#"
      GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET: "#VAR:app_grafana_oauth2_client_secret:VAR#"
      GF_AUTH_GENERIC_OAUTH_SCOPES: openid email profile offline_access roles
      GF_AUTH_GENERIC_OAUTH_EMAIL_ATTRIBUTE_PATH: email
      GF_AUTH_GENERIC_OAUTH_LOGIN_ATTRIBUTE_PATH: username
      GF_AUTH_GENERIC_OAUTH_NAME_ATTRIBUTE_PATH: full_name
      GF_AUTH_GENERIC_OAUTH_AUTH_URL: "#VAR:app_grafana_oauth2_base_url:VAR#/protocol/openid-connect/auth"
      GF_AUTH_GENERIC_OAUTH_TOKEN_URL: "#VAR:app_grafana_oauth2_base_url:VAR#/protocol/openid-connect/token"
      GF_AUTH_GENERIC_OAUTH_API_URL: "#VAR:app_grafana_oauth2_base_url:VAR#/protocol/openid-connect/userinfo"
      GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH: "contains(roles[*], 'admin') && 'Admin' || contains(roles[*], 'editor') && 'Editor' || 'Viewer'"
      GF_SMTP_ENABLED: "true"
      GF_SMTP_HOST: "#VAR:mailjet_host:VAR#:#VAR:mailjet_port_starttls:VAR#"
      GF_SMTP_USER: "#VAR:mailjet_username:VAR#"
      GF_SMTP_PASSWORD: "#VAR:mailjet_password:VAR#"
      GF_SMTP_FROM_ADDRESS: "grafana.#VAR:inventory_hostname:VAR#@#VAR:general_domain_default:VAR#"
      GF_SMTP_FROM_NAME: "Grafana on #VAR:inventory_hostname:VAR#"
      GF_SMTP_STARTTLS_POLICY: "MandatoryStartTLS"

  loki:
    <<: *defaults
    image: docker.io/grafana/loki:latest
    container_name: loki
    command: -config.file=/loki-config/config.yaml
    user: "#VAR:ansible_real_user_id:VAR#:#VAR:ansible_real_group_id:VAR#"
    networks:
      - caddy
      - loki
    environment:
      TZ: "#VAR:general_timezone:VAR#"
    volumes:
      - ./loki:/loki-config:ro
      - "#VAR:general_path_appdata:VAR#/loki/data:/loki"
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 8G

  # =========================
  # =         BACKUP        =
  # =========================
  borgmatic:
    <<: *defaults
    profiles: ["scheduled"]
    build: ./borgmatic
    container_name: borgmatic
    cap_add:
      - SYS_ADMIN # Required for borg mount
    volumes:
      # === Backup locations ===
      # Zabbix
      - "zabbix-db-dump:/mnt/source/zabbix/dbdump:ro"
      # === Config and cache ===
      - "./borgmatic/common/global.txt:/common/global.yml"
      - "./borgmatic/common/repo.txt:/common/repo.yml"
      - "./borgmatic/apps:/etc/borgmatic.d"
      - "./borgmatic/ssh:/root/.ssh"
      - "#VAR: general_path_appdata :VAR#/borgmatic/borgmatic/config:/root/.config/borg"
      - "#VAR: general_path_appdata :VAR#/borgmatic/borgmatic/cache:/root/.cache/borg"
      - "#VAR: general_path_appdata :VAR#/borgmatic/borgmatic/restore:/mnt/restore"
    devices:
      - "/dev/fuse:/dev/fuse" # Required for borg mount
    security_opt:
      - "apparmor=unconfined" # Required for borg mount
    environment:
      TZ: "#VAR: general_timezone :VAR#"
      BACKUP_CRON: "false"
      BORG_PASSPHRASE: "#VAR: app_borgmatic_borg_passphrase :VAR#"
      BORG_HOSTNAME_IS_UNIQUE: "yes" # Automatically removes stale locks
      BORG_HOST_ID: "#VAR: inventory_hostname :VAR#"
    deploy:
      resources:
        limits:
          cpus: "#VAR: ansible_processor_nproc :VAR#"
          memory: 4G

#################################################################
#                            NETWORKS                           #
#################################################################
networks:
  caddy:
    name: caddy

  loki:
    name: loki

  prometheus:
    name: prometheus

  zabbix:
    name: zabbix

#################################################################
#                            VOLUMES                            #
#################################################################
volumes:
  caddy-access-logs:
  caddy-config:
  caddy-data:
  zabbix-db-dump:
